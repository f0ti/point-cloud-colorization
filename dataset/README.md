# Dataset

This module contains all scripts needed for generating the dataset.

## Melbourne

For the ![melbourne dataset](https://discover.data.vic.gov.au/dataset/city-of-melbourne-3d-point-cloud-2018) we have the following structure:

- ```raw_data``` - extracted .las data and tiles from zip
- ```las_data``` - contains the .las data
- ```rgb_data``` - contains only the RGB data extracted from tiles saved as numpy tensors
- ```xyz_data``` - contains only the XYZ data extracted from tiles saved as numpy tensors
- ```tiles_data``` - contains the tiles extracted from .las data in binary

## Dataset Generation Process

This guide outlines the steps to generate and prepare a new dataset, ensuring it is properly structured and filtered for further use.

1. **Create Dataset Directory:**
   - Start by creating a dedicated directory for your new dataset under the `dataset` directory. This directory will store all related files and data. Example:
     ```bash
     mkdir dataset/my_dataset
     ```

2. **Transfer Dataset Archive:**
   - Move the `.zip` file containing the dataset, generated by the C++ scripts, into the newly created `dataset/my_dataset` directory. This file should include all the necessary images and coordinates files.

3. **Extract Dataset Files:**
   - Extract the contents of the `.zip` file using the provided shell script.
     ```bash
     ./run_unzip.sh zip_file.zip
     ```
   - After extraction, the directory should contain all the raw data needed for further processing.

4. **Generate RGB and XYZ Data:**
   - Navigate to the `utils` directory, which contains various utility scripts for processing the dataset:
     ```bash
     cd utils
     ```
   - Run the `extract_tiles.py` script to process the raw data. This script generates `rgb_data` (containing image tiles) and `xyz_data` (containing corresponding XYZ coordinates) directories, both containing the data in `.npy` format
     ```bash
     ./extract_tiles.py my_dataset
     ```

5. **Create Dataset Image Embeddings:**
   - To filter out irrelevant or outlier images, generate embeddings for a sample of 1000 images using an image model of your choice in the dataset using the following command:
     ```bash
     python dataset_embedding.py
     ```
   - Recommended model: `gluon_inception_v3_avg_dataset_embedding.pt`

6. **Identify and Filter Outlier Images:**
   - Run the `compare_images.py` script to compare each image's embedding with the average embedding of the dataset. This script generates a blacklist of images that significantly differ from the average, indicating potential outliers:
     ```bash
     python compare_images.py
     ```
   - Review the blacklisted images manually. Once reviewed, remove these images from the `rgb_data` and `xyz_data` directories by executing:
     ```bash
     python filter_from_blacklist.py
     ```

7. **Cluster and Split Dataset:**
   - Finally, split the dataset into training and testing subsets. The `image_clustering_data_split.py` script clusters images based on their color histograms, ensuring an even distribution between `train` and `test` directories for both `rgb_data` and `xyz_data`:
     ```bash
     python image_clustering_data_split.py
     ```
   - After this step, the dataset will be organized into `train` and `test` subsets, ready for model training and evaluation.
